# [2nd] ML Notes

This note is based on online open course released on youtube of  Prof. Hung Yi, Lee, NTU(National Taiwan University), at 2017

## Linear Regression

---

[Linear Regression](https://www.notion.so/Linear-Regression-f95e808dd72345d88985088c14833cba)

### The Model and General View

    The purpose of Linear Regression is to use the past experience to predict the future, in which we estimate a function $f(x)$ describe the complicated real world. Since it's called 'Linear' Regression, the equation we are estimating this time is $f(x) = wx+b$, a linear equation. To optimize the estimated function, we plug the input $x$ into the equation, and compare its output, the predicted value, to the actual data $\hat{y}$ given for each input to calculate the 'error' of the estimated equation, and our goal is to minimize this error. This estimated equation is

$$f(x_1,x_2,...,x_n)=b+\sum w_nx_n=y \\ 
error =y \leftrightarrow \hat{y}$$

- $\{x_1,x_2,...,x_n\}$ the data given
- $(y,\hat{y})$ the estimated value and the actual data
- $f(x_1,x_2,...,x_n)$ the estimated function
- $(w_n,b_n)$ the parameters of the equation, the weights and biases

![%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_5.43.46_PM.png](%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_5.43.46_PM.png)

The plot here demonstrates the essence of Linear Regression, where the red line is the estimated function that best fits the data points(blue) given

### The Goodness of the Function

    The essence that optimized the estimated equation is to find the best parameter $(w,b)$ to minimize the error and to let the output of the function to fit the given value $\hat{y}$. To do so, we define another function, the loss function $L(f(w,b))$, to define the error of various parameters $(w,b)$ in different functions $f$, the goodness of the function. This loss function could be various, such as MSE(Mean Square Error), generally saying is to take the sum of difference of the predicted and actual value and take the square of it.

$$L(f)=L(w,b)=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2=\frac{1}{n}\sum_{i=1}^n(f_i(w,b)-\hat{y_i})^2$$

![%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_5.43.46_PM.png](%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_5.43.46_PM.png)

The error that defines by the loss function is the sum of the distance between the predicted function(red line) and all of the data points(blue)
                                                                                                - Picture generated by own code

    The mission of optimizing becomes to minimize the loss function, so we can find the best function $f^*$ that represent the trend of the data and the relationship of input and prediction.

$$f^*= arg\min L(f)=arg\min_{w,b}L(f(w,b)) $$

The method to minimize the equation  is called 'Gradient Descent', we use the gradient of the function to gradually minimize the equation and find the best function $f^*$.

### Optimization, Gradient Descent

**Gradient Review:**

 $\nabla f =(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}) = \frac{\partial f}{\partial x}\vec{i}+\frac{\partial f}{\partial y}\vec{j}+\frac{\partial f}{\partial z}\vec{k}$

Taking the derivate of a function and get the vector of the function's direction (where the direction is heading toward to). 

    Gradient Descent, just like we talk before, is a method to minimize the loss function in linear regression and most of other machine learning topics. At gradient descent, we can think of the loss function as  graph with $(x,y)$ as its parameter $(w,b)$, and $z$ axis as the value of the loss function, essentially look like this:

![%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Untitled.png](%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Untitled.png)

Picture from Wikipedia - Gradient Descent

    This graph is the graph of the loss function, we can see, the minima of the loss function is clearly where the $z$ is the smallest, so our goal is to making sure that we can walk from any random points on the graph to the lowest point on the graph. As you can see, the minima of the graph will be located at where else where is steep around it, that means we can use this relationship to find the minima, and this is the idea of gradient descent. The essence of gradient descent is to slowly sliding the parameters to the minima by how 'steep' is the current point is, which can be achieve by gradient. 

$$\theta_{i+1} = \theta_i-\eta\frac{\partial}{\partial \theta}L(\theta)$$

    This is the formula for gradient descent, the updated parameter is based on the the previous parameter and the derivate of the function with a coefficient $\eta$  eta, the learning rate, which controls the speed of the update. As we take more steps, $\frac{\partial L}{\partial \theta}$ will gradually converge to 0, exactly the minima behaves, the parameter will stop updating and stays there. Take some time for the equation, we can see that it is exactly the situation we discussed, we make use of the steepness of the equation to update the parameters.

![%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_8.15.35_PM.png](%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_8.15.35_PM.png)

The picture of gradient descent
            - Picture from builtin.com

    The reason we call this gradient descent is because when we write the formula of it for all parameters at once, it'll look like this

$$\begin{bmatrix}\theta_0^\prime\\\theta_1^\prime\\...\\\theta_n^\prime\end{bmatrix} = \begin{bmatrix}\theta_0\\\theta_1\\...\\\theta_n\end{bmatrix}-\eta\begin{bmatrix}\frac{\partial L(\theta_0)}{\partial \theta_0}\\\frac{\partial L(\theta_1)}{\partial \theta_1}\\...\\\frac{\partial L(\theta_n)}{\partial \theta_n}\end{bmatrix}$$

which looks exactly like the definition of the gradient!

   Of course, this kind of vanilla gradient descent will exist some problems, such as how do you decide the learning rate $\eta$, or problems of stuck at local minima or paddle point, since there also exist points with 0 gradient, or will this method even converges, and these will be discuss in future sections.

### Problems, Under and Over Fitting

    There is a problem of linear model, when the distribution of the data don't even look like a linear function, then you error will still be large since you can never estimate a linear function to higher-order function. This situation is called 'Under-fitting', the problem will occur whenever there are only so little parameters, you can never estimate the real-world function. And this issue will carries through out the whole machine learning journey. The answer for Under-fitting is to add more parameters, to let the model becomes a higher-order function, so it can estimate different kinds of real world functions. With that in mind, we can rewrite the function of the model as this: 

$$f =w_ix^n+w_{i-1}x^{n-1}+...+w_1x+b$$

and so, the parameters we are optimizing are plenty whole lots more than $(w,b)$, which can give us some hope on solving the under-fitting problem.

    However, it is not necessarily  good to have too many parameters, though with more parameters, it do can estimate more functions, but sometimes the function may "overfit".

![%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_10.14.14_PM.png](%5B2nd%5D%20ML%20Notes%200f00bebe57854904bf3d266f6948d6d1/Screen_Shot_2021-07-28_at_10.14.14_PM.png)

Picture from Prof. Hung Yi, Lee's PPT 

As you can see from the picture above, the predicted function(red line) clearly overfitted, though the redline is fairly close to all the data points given, you can see that the actual trend of the data is clearly not dropping and rising, but only a line from bottom-left to top-right. This situation is the overfitting, due to the urge of  the gradient descent that want to minimize its error, sometimes it might ignore the original job to predict the trend. It may fit all of the given data(training data), it won't fit the real world(test data). 

    In the field of machine learning, you will tune the numbers of the parameters, to balance the overfitting and under-fitting problem, and these kinds of so-called hyper-parameters tuning is abundant, will be the most annoying and tedious process in practice .

## Error of Machine Learning

---

[Error of Machine Learning](https://www.notion.so/Error-of-Machine-Learning-3f2fc6b729ea4ca081e4495088ea36c4)

## Theory of Gradient Descent

---

## Classification

---

## Logistic Regression and Neuron

---

## Deep Learning

---

## Backpropagation

---

## Tricks for Training Deep Learning Model

---

## Convolution Neural Network

---

## Semi-Supervised Learning

---

## Linear Model

---

## Word Embedding

---

## Neighbor Embedding

---

## Auto-Encoder

---

## Generative Adversarial Network

---

## Recurrent Neural Network and Long-Short-Term-Memory

---

## Support Vector Machine

---